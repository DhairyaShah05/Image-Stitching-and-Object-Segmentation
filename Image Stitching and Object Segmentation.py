# -*- coding: utf-8 -*-
"""dshah05_proj2_enpm673.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EjSzxzDziMnaXxIyaKrw7ToAp2NYTmWv
"""

# Importing Google Drive
from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

# Importing Dependencies
import cv2
import numpy as np
from matplotlib import pyplot as plt
from google.colab.patches import cv2_imshow

# Function to Extract Frames
def extract_frames(video_path):
    video = cv2.VideoCapture(video_path)
    frames = []
    while video.isOpened():
        ret, frame = video.read()
        if not ret:
            break
        frames.append(frame)
    video.release()
    return frames

# Importing Video
video_path = "/content/drive/MyDrive/ENPM673/Project_2/proj2_v2.mp4"
frames = extract_frames(video_path)

"""The function `is_blurry` checks if a frame is blurry by converting it to grayscale, applying an edge-detection kernel, and comparing the variance to a threshold. If the variance is below the threshold, it returns `True`; otherwise, it returns `False`."""

# Function to Determine if the Frame is Blurry
def is_blurry(frame, threshold = 50):

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Defining a custom kernel for blurring
    kernel = np.array([[0, 1, 0],
                       [1,  -4, 1],
                       [0, 1, 0]])

    # Applying the kernel for sharpening
    img_var = cv2.filter2D(gray, -1, kernel).var()

    return img_var < threshold

# Function to Segment Background
def segment_background(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
    return thresh

# Function to Detect Edges (using Canny)
def detect_edges(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
    # Detect edges using Canny
    edges = cv2.Canny(gray, 200, 250)
    return edges

def extract_lines(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
    lines = cv2.HoughLinesP(frame, 1, np.pi/180, threshold=100, minLineLength=80, maxLineGap=10)
    return lines

"""The below functions filter out short lines from a list of lines and compute the intersections between those lines."""

# Function to Filter Lines and Compute Intersections
def filter_lines(lines):
    filtered_lines = []
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            if length > 100:  # Filtering out short lines
                filtered_lines.append(line)
    return filtered_lines

def compute_intersections(lines):
    intersections = []
    for i in range(len(lines)):
        for j in range(i+1, len(lines)):
            line1 = lines[i][0]
            line2 = lines[j][0]
            x1, y1, x2, y2 = line1
            x3, y3, x4, y4 = line2
            denominator = (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4)
            if denominator != 0:
                intersection_x = ((x1*y2-y1*x2)*(x3-x4) - (x1-x2)*(x3*y4-y3*x4)) / denominator
                intersection_y = ((x1*y2-y1*x2)*(y3-y4) - (y1-y2)*(x3*y4-y3*x4)) / denominator
                intersections.append((intersection_x, intersection_y))
    return intersections

# Function to Verify Corners
def verify_corners(frame, intersections):
    corners = []
    for intersection in intersections:
        x, y = intersection
        corners.append((int(x), int(y)))
    return corners

# Function to Overlay Lines and Corners
def overlay_lines_and_corners(frame, lines, corners):
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 4)
    if corners is not None:
        for corner in corners:
            x, y = corner
            cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)
    return frame

"""The below code iterates over a set of frames, determining if each frame is blurry or not, processing non-blurry frames by segmenting, detecting edges, extracting lines, filtering lines, computing intersections, verifying corners, overlaying lines and corners, and finally adding processed frames to `output_frames`; it also counts the number of blurry frames and total frames processed."""

# Implementation
output_frames = []
blurry_frame_count = 0
total_frame_count = 0
for frame in frames:
    if is_blurry(frame):
        blurry_frame_count += 1
        total_frame_count += 1
    else:
        total_frame_count += 1
        segmented_frame = segment_background(frame)
        edges = detect_edges(segmented_frame)
        lines = extract_lines(edges)
        filtered_lines = filter_lines(lines)
        intersections = compute_intersections(filtered_lines)
        corners = verify_corners(frame, intersections)
        output_frame = overlay_lines_and_corners(frame, filtered_lines, corners)
        # cv2_imshow(output_frame)
        key = cv2.waitKey(100)
        if key == ord('q'):
            break
        output_frames.append(output_frame)

print("Number of blurry frames:", blurry_frame_count)
print("Number of total frames:", total_frame_count)

# Write the output video
output_video_path = "/content/drive/MyDrive/ENPM673/Project_2/output_video.mp4"
height, width, _ = output_frames[0].shape
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))
for frame in output_frames:
    out.write(frame)
out.release()

print("Number of blurry frames:", blurry_frame_count)
print("Number of total frames:", total_frame_count)

"""Question 2

**Justification for using SIFT**

 **SIFT (Scale-Invariant Feature Transform)** is used due to its robustness to variations in scale, rotation, and illumination. As images captured for panoramas may have variations in perspective, lighting conditions, and scale.
  SIFT features are distinctive and invariant to these changes, making them ideal for matching corresponding points in overlapping images accurately and provide a high level of repeatability, meaning they are likely to be detected in multiple views of the same scene.
"""

# Function to Extract Features using SIFT (Scale-Invariant Feature Transform) for feature extraction)
def extract_features(image):
    # Use SIFT (Scale-Invariant Feature Transform) for feature extraction
    sift = cv2.SIFT_create()
    key_points, d = sift.detectAndCompute(image, None)
    return key_points, d

"""
The code performs feature matching between two sets of descriptors using FLANN (Fast Library for Approximate Nearest Neighbors) algorithm with a KNN (K-Nearest Neighbors) approach, filtering matches based on a distance ratio criterion, and returning the matched features.

FLANN (Fast Library for Approximate Nearest Neighbors) is an efficient library for finding approximate nearest neighbors in high-dimensional spaces, commonly used in computer vision for tasks such as feature matching and image retrieval."""

def feature_matching(d1, d2):
    # Use Flann-based Matcher with KNN matching for feature matching
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(d1, d2, k=2)

    perfect_matches = []
    for m, n in matches:
        if m.distance < 0.8 * n.distance:
            perfect_matches.append(m)
    return perfect_matches

"""The function `draw_matches` visualizes key point matches between two input images using OpenCV's `drawMatches` function and displays the result."""

# Function to draw matches and show them
def draw_matches(image1, image2, key_points1, key_points2, matches):
    # Visualize the matches between two images
    img_matches = cv2.drawMatches(image1, key_points1, image2, key_points2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    cv2_imshow(img_matches)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

"""The function `combine_images` stitches two input images together by extracting corresponding key points from matches, computing a homography using RANSAC, and applying warpPerspective to merge the images seamlessly."""

# Function to Stitch Images
def combine_images(key_points1, key_points2, matches, image1, image2):
    # Extract corresponding points from the matches
    points1 = np.float32([key_points1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
    points2 = np.float32([key_points2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

    # Use RANSAC to compute homography
    H, _ = cv2.findHomography(points1, points2, cv2.RANSAC, 6.0)

    # Use warpPerspective to stitch two images together
    result = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))
    result[0:image2.shape[0], 0:image2.shape[1]] = image2
    return result

# Function to Process any image
def process_image(image_path):
    # Read image
    image = cv2.imread(image_path)
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Extract features
    kpts, d = extract_features(gray)
    return image, gray, kpts, d

# Process images
image_paths = ['/content/drive/MyDrive/ENPM673/Project_2/image1.jpg',
               '/content/drive/MyDrive/ENPM673/Project_2/image2.jpg',
               '/content/drive/MyDrive/ENPM673/Project_2/image3.jpg',
               '/content/drive/MyDrive/ENPM673/Project_2/image4.jpg']

images = []
grays = []
key_pointss = []
ds = []
for path in image_paths:
    image, gray, key_points, d = process_image(path)
    images.append(image)
    grays.append(gray)
    key_pointss.append(key_points)
    ds.append(d)

# Match features between consecutive images
matches_list = []
for i in range(len(ds) - 1):
    matches = feature_matching(ds[i], ds[i+1])
    matches_list.append(matches)

# Visualize matches
for i in range(len(matches_list)):
    draw_matches(grays[i], grays[i+1], key_pointss[i], key_pointss[i+1], matches_list[i])

# Combine Images to create Panorama
panorama = images[0]
for i in range(len(matches_list)):
    panorama = combine_images(key_pointss[i], key_pointss[i+1], matches_list[i], panorama, images[i+1])

# Display the panorama
cv2_imshow(panorama)
cv2.waitKey(0)
cv2.destroyAllWindows()

"""Question 2 B

Panoramic mosaicing, also known as panorama stitching, works better when the camera is only allowed to rotate at its camera center due to several reasons:

1. **Minimization of Parallax Errors**: Parallax occurs when objects at different distances appear to shift relative to each other when viewed from different positions. When rotating the camera around its center, the perspective of the scene remains consistent, reducing parallax errors.

2. **Preservation of Scene Geometry**: Rotating the camera around its center preserves the geometry of the scene, maintaining the relationships between objects and their proportions. This preservation is crucial for accurately aligning and blending images in the panorama.

3. **Simplification of Image Registration**: Rotation-based registration methods are computationally simpler and more robust compared to other types of transformations, such as translation or affine transformations.

4. **Consistency of Projection**: When rotating the camera around its center, the projection of the scene onto the image sensor remains consistent across all captured frames. This consistency simplifies the task of mapping image pixels to a common reference frame during panorama stitching.

5. **Reduction of Stitching Artifacts**: Rotating the camera around its center minimizes stitching artifacts, such as ghosting or misalignment, which can occur when blending images captured from different viewpoints.
"""